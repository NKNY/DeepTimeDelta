{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data from Table 1 to be used for statistical tests by R scripts in this folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as ss\n",
    "\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure everything is rounded to 4 significant digits\n",
    "num_sign_digits = 4\n",
    "pd.options.display.float_format = ('{:,.' + str(num_sign_digits) + 'f}').format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data in the format described more precisely in RQ3/general/4a_submit_u_evaluator_non_rtl.ipynb\n",
    "input_path = \"/Users/nknyazev/Documents/Delft/Thesis/temporal/data/results/best_runs.json\"\n",
    "rtl_results_path = \"/Users/nknyazev/Documents/Delft/Thesis/temporal/data/results/rtl/offline201909.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prefix(dictionary, prefix, sep=\"_\"):\n",
    "    return {prefix + sep + k:v for k,v in dictionary.items()}\n",
    "# https://stackoverflow.com/questions/6027558/flatten-nested-python-dictionaries-compressing-keys\n",
    "def flatten(d, parent_key='', sep='_'):\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = parent_key + sep + k if parent_key else k\n",
    "        if isinstance(v, collections.MutableMapping):\n",
    "            items.extend(flatten(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "def convert_types(df):\n",
    "    cols = df.columns\n",
    "    types = [int, float, str]\n",
    "    for col in cols:\n",
    "        for t in types:\n",
    "            try:\n",
    "                df[col] = df[col].astype(t)\n",
    "                break\n",
    "            except ValueError as e:\n",
    "                pass\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename metrics + model_id, applied to df columns\n",
    "def metric_renamer(col_name):\n",
    "    metric_to_report_name = {\n",
    "        \"mrr\": \"MRR@20\",\n",
    "        \"u_mrr\": \"UserMRR@20\",\n",
    "        \"recall\": \"Recall@20\",\n",
    "        \"u_recall\": \"UserRecall@20\",\n",
    "        \"model_id\": \"model\"\n",
    "    }\n",
    "    return metric_to_report_name[col_name.replace(\"test\", \"\")\\\n",
    "        .replace(\"validation\", \"\")\\\n",
    "        .replace(\"_all_\", \"\")]\n",
    "\n",
    "# Group data per col (e.g. per dataset) and find the highest metric among the models\n",
    "def bold_col_max_per_col(df, groupby_col=\"Dataset\"):\n",
    "    df_copy = df.copy()\n",
    "    # Iterate over each metric\n",
    "    for c in df.columns:\n",
    "        # Find idx where the group's value is the highest\n",
    "        max_idx = df_copy.groupby(groupby_col)[c].transform(max) == df[c]\n",
    "        # Replace those values with bold text\n",
    "        df_copy[c][max_idx] = df_copy[c][max_idx].apply(lambda x: \"\\\\textbf{\" + (\"{0:.\" + str(num_sign_digits) + \"f}\").format(x) + \"}\")\n",
    "    return df_copy\n",
    "\n",
    "# Rename datasets from working names to report names (or any other val in the index col)\n",
    "def map_level(df, d, level=0, inplace=True):\n",
    "    index = df.index\n",
    "    index.set_levels([[d.get(item, item) for item in names] if i==level else names\n",
    "                      for i, names in enumerate(index.levels)], inplace=inplace)\n",
    "\n",
    "# Get count of cols in index + df.columns and create a string like c|c|c|c|c for alignment inside columns\n",
    "def get_table_column_format(df, borders=True, positioning=\"c\"):\n",
    "    num_cols = len(df.index.names) + len(df.columns)\n",
    "    table_column_format = (positioning + \"|\" if borders else positioning)*(num_cols-1) + positioning\n",
    "    return table_column_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_path) as input_file:\n",
    "    results = json.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in results.items():\n",
    "    results[k] = {**v[\"run_params\"], **flatten(v[\"results\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(results.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = convert_types(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtl_df = pd.read_json(open(rtl_results_path)).transpose()\n",
    "rtl_df.index = rtl_df.index.map(lambda x: x.split(\" \")[-1])\n",
    "rtl_df = rtl_df.reset_index().rename({\"index\": \"model_id\"}, axis=1)\n",
    "rtl_df = rtl_df.rename(lambda x: \"test_all_\" + x if x != \"model_id\" else x, axis=1)\n",
    "rtl_df[rtl_df.columns] = rtl_df[rtl_df.columns].astype(str) \n",
    "rtl_df[\"dataset\"] = \"rtl\"\n",
    "rtl_df = convert_types(rtl_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine results from rtl and offline data as those kept in different locations\n",
    "df = pd.concat([df, rtl_df], sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to keep\n",
    "# cols_to_keep = [c for c in df.columns if c.startswith(\"test_all_\")] + [\"model_id\"]\n",
    "cols_to_keep = [\"test_all_recall\", \"test_all_mrr\", \"test_all_u_recall\", \"test_all_u_mrr\", \"model_id\"]\n",
    "# Make MultiIndexed DF (looks nicer) and rename columns\n",
    "df2 = df.copy()\\\n",
    "        .set_index([\"dataset\", \"model_id\"])\\\n",
    "        .sort_index()[[c for c in cols_to_keep if c != \"model_id\"]]\\\n",
    "        .rename(metric_renamer, axis=1)\n",
    "# Rename index\n",
    "df2.index.set_names([\"Dataset\", \"model\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = df2.columns\n",
    "datasets = df2.index.get_level_values(0).unique()\n",
    "for metric in metrics:\n",
    "    output_path = os.path.join(output_root, \"{}.csv\".format(metric))\n",
    "    d = df2[metric].round(5)\n",
    "    to_export = d.unstack()\n",
    "    to_export.rename(lambda x: \"m{}\".format(x), inplace=True, axis=1)\n",
    "    to_export.to_csv(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model_ids and which equations modified\n",
    "model_id_path = os.path.join(output_root, \"model_ids.csv\")\n",
    "equation_path = os.path.join(output_root, \"equations.csv\")\n",
    "\n",
    "model_id_df = pd.DataFrame([\"m{}\".format(x) for x in range(8)], columns=[\"model_id\"])\n",
    "equation_df = pd.DataFrame([[0,0,0],[1,0,0],[0,1,0],[0,0,1],[1,1,0],[1,0,1],[0,1,1],[1,1,1]], columns=[\"eq1\", \"eq2\", \"eq3\"], dtype=bool)\n",
    "\n",
    "model_id_df.to_csv(model_id_path, index=False)\n",
    "equation_df.to_csv(equation_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also export the whole array in non-wide format to be used by normal anova for example\n",
    "non_wide_path = os.path.join(output_root, \"nonwide.csv\")\n",
    "\n",
    "df3 = df2.copy()\n",
    "df3.rename(lambda x: x[:-3], inplace=True, axis=1)\n",
    "repeated_equations = pd.concat([equation_df]*len(df3.index.get_level_values(0).unique()))\n",
    "df3 = df3.reset_index().join(repeated_equations.reset_index()).drop(\"index\", axis=1)\n",
    "df3[\"model\"] = df3[\"model\"].apply(lambda x: \"m\"+str(x))\n",
    "\n",
    "df3.to_csv(non_wide_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis_p3",
   "language": "python",
   "name": "thesis_p3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
