{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Extract results from user_evaluators\n",
    "\n",
    "Loads all specified u_evaluators for the given DATASET, SUBSETS. Combines them into one csv file with columns specifying the subset, model_id, metric and the numerator and denominator values.\n",
    "\n",
    "Requires:\n",
    "* u_evaluator_folders.json - json file formatted as:\n",
    "> 'unique_run_id': {'dataset': 'lastfm_10_pc',\n",
    "   'model_id': 6,\n",
    "   'path': 'path/to/last/common/ancestor/folder/of/all/u_evaluators'}\n",
    "   \n",
    "   Can also specify the folder structure to be able load individual evaluators from the specified path.\n",
    "* Project code to be located at the path specified by `code_root`\n",
    "\n",
    "Returns:\n",
    "* user_evaluator_results.json containing each user's numerator and denominator as described above. Note that the empty users are not discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"lastfm_10_pc\"\n",
    "LOCATION = \"local\"\n",
    "SUBSETS = [\"validation\",\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOCATION == \"local\":\n",
    "    code_root = \"/Users/nknyazev/Documents/Delft/Thesis/temporal/code/model\"\n",
    "    pickle_paths_path = \"/Users/nknyazev/Downloads/u_evaluator_folders.json\"\n",
    "    pickle_template = \"3/{}_u_evaluator\"\n",
    "    output_root = \"/Users/nknyazev/Documents/Delft/Thesis/temporal/data/results/RQ3\"\n",
    "    \n",
    "elif LOCATION == \"server\":\n",
    "    code_root = \"/home/nfs/nknyazev/thesis/Thesis/model\"\n",
    "    pickle_paths_path = \"/home/nfs/nknyazev/thesis/data/results/u_evaluator_folders.json\"\n",
    "    pickle_template = \"3/{}_u_evaluator\"\n",
    "    output_root = \"/tudelft.net/staff-bulk/ewi/insy/MMC/nknyazev/RQ3\"\n",
    "\n",
    "\n",
    "elif LOCATION == \"rtl\":\n",
    "    code_root = \"/home/ec2-user/SageMaker/thesis/Thesis/model\"\n",
    "    pickle_paths_bucket = \"ci-data-apps\"\n",
    "    pickle_paths_key = \"norman/sagemaker/thesis/offline-evaluation/results/u_evaluator_folders.json\"\n",
    "    pickle_template = \"all/{}_u_evaluator\"\n",
    "    output_root = \"s3://ci-data-apps/norman/sagemaker/thesis/offline-evaluation/RQ3\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend the path as otherwise running into import issues during unpickling\n",
    "sys.path.append(code_root)\n",
    "\n",
    "output_filename = \"user_evaluator_results.json\"\n",
    "output_folder = os.path.join(output_root, DATASET)\n",
    "output_path = os.path.join(output_folder, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle_params = {\n",
    "#     '14': {'dataset': 'lastfm_10_pc',\n",
    "#   'model_id': 4,\n",
    "#   'path': '/Users/nknyazev/Downloads/evaluator_test/lastfm_10_pc/500/4/0.5/0.5/0.5/0/tf.sigmoid/1147'},\n",
    "#  '15': {'dataset': 'lastfm_10_pc',\n",
    "#   'model_id': 6,\n",
    "#   'path': '/Users/nknyazev/Downloads/evaluator_test/lastfm_10_pc/500/6/0.5/0.5/0.5/1/tf.nn.relu/1142'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_pickle_paths = defaultdict(dict)\n",
    "\n",
    "if LOCATION == \"rtl\":\n",
    "    s3 = boto3.resource('s3')\n",
    "    content_object = s3.Object(pickle_paths_bucket, pickle_paths_key)\n",
    "    file_content = content_object.get()['Body'].read().decode('utf-8')\n",
    "    pickle_params = json.loads(file_content)\n",
    "    tmp_folder = \"/tmp\"\n",
    "    for d in pickle_params.values():\n",
    "        if d[\"dataset\"] != DATASET:\n",
    "            continue\n",
    "        for subset in SUBSETS:\n",
    "            model_id = str(d['model_id'])\n",
    "            model_tmp_folder = os.path.join(tmp_folder, model_id)\n",
    "            pickle_filename = pickle_template.format(subset)\n",
    "            pickle_s3_path = os.path.join(d['path'], pickle_filename)\n",
    "            pickle_local_path = os.path.join(model_tmp_folder, pickle_filename)\n",
    "            _ = subprocess.call(['aws', 's3', 'cp', pickle_s3_path, pickle_local_path])\n",
    "\n",
    "            local_pickle_paths[subset][model_id] = pickle_local_path\n",
    "else:\n",
    "    with open(pickle_paths_path) as input_file:\n",
    "        pickle_params = json.load(input_file)\n",
    "    for d in pickle_params.values():\n",
    "        if d[\"dataset\"] != DATASET:\n",
    "            continue\n",
    "        for subset in SUBSETS:\n",
    "            model_id = str(d['model_id'])\n",
    "            pickle_filename = pickle_template.format(subset)\n",
    "            pickle_local_path = os.path.join(d['path'], pickle_filename)\n",
    "            local_pickle_paths[subset][model_id] = pickle_local_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickles = defaultdict(dict)\n",
    "for model_id, subsets in local_pickle_paths.items():\n",
    "    for subset, pickle_path in subsets.items():\n",
    "        pickles[subset][model_id] = pickle.load(open(pickle_path, \"rb\"))\n",
    "        if LOCATION == \"rtl\":\n",
    "            os.remove(pickle_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {subset: {model_id: {metric: ([num_u1, num_u2,...], [denom_u1, denom_u2,...]),...}}}\n",
    "num_denoms = {s: {model_id: v._num_denom for model_id, v in evaluators.items()} for s, evaluators in pickles.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to one pandas array\n",
    "pre_df_dict = {\n",
    "    (model_id, subset, metric): [num, denom]\n",
    "    for model_id, subsets in num_denoms.items() \n",
    "    for subset, metrics in subsets.items() \n",
    "    for metric, (num, denom) in metrics.items() \n",
    "}\n",
    "\n",
    "values = np.concatenate(list(pre_df_dict.values()), axis=1).T\n",
    "index_values = [(*k,i) for k, v in pre_df_dict.items() for i in range(v[0].shape[0])]\n",
    "index = pd.MultiIndex.from_frame(pd.DataFrame(index_values, columns=[\"model_id\", \"subset\", \"metric\", \"uid\"]))\n",
    "\n",
    "output_df = pd.DataFrame(values, index=index, columns=[\"num\", \"denom\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOCATION != \"rtl\":\n",
    "    output_df.to_csv(output_path, sep=\"\\t\")\n",
    "else:\n",
    "    tmp_folder = \"/tmp\"\n",
    "    tmp_path = os.path.join(tmp_folder, output_filename)\n",
    "    output_df.to_csv(tmp_path, sep=\"\\t\")\n",
    "    _ = subprocess.call(['aws', 's3', 'cp', tmp_path, output_path])\n",
    "    os.remove(tmp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis_p3",
   "language": "python",
   "name": "thesis_p3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
