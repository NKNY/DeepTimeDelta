{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Extract dt stats and item consumption per user\n",
    "\n",
    "Requires:\n",
    "* X.npy files for each of the dataset's subsets.\n",
    "\n",
    "Returns:\n",
    "* A csv for the combined dataset (from all subsets) with dt stats (e.g. median, mean) for each user as well a dict with all items and their counts that have been consumed by the user "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCATION = \"local\"\n",
    "DATASET = \"lastfm_10_pc\"\n",
    "SUBSETS = [\"train\",\"validation\", \"test\"]\n",
    "STATS_TO_CALCULATE = [\"median\", \"mean\", \"count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import subprocess\n",
    "import sys\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from evaluation/2_evaluate_exports/RQ2.1/prev_current_dist_to_recs_vs_dt_log_bins.ipynb\n",
    "\n",
    "def randomString(stringLength=10):\n",
    "    \"\"\"Generate a random string of fixed length \"\"\"\n",
    "    letters = string.ascii_lowercase\n",
    "    return ''.join(random.choice(letters) for i in range(stringLength))\n",
    "\n",
    "def load_arrays(root, *args):\n",
    "    \n",
    "    if len(args) > 0 and not root.startswith(\"s3\"):\n",
    "        return {k: np.load(os.path.join(root, k + \".npy\")) for k in args}\n",
    "    outputs = {}\n",
    "    temp_path = os.path.join(randomString())\n",
    "    subprocess.call([\"mkdir\", \"-p\", temp_path])\n",
    "    for a in args:\n",
    "        local_path = os.path.join(temp_path, a)\n",
    "        s3_path = os.path.join(root, a)\n",
    "        subprocess.call([\"mkdir\", \"-p\", local_path])\n",
    "        subprocess.call([\"aws\", \"s3\", \"cp\", s3_path, local_path, \"--recursive\"])\n",
    "        file_names = sorted([os.path.join(local_path, x) for x in next(os.walk(local_path))[-1]])\n",
    "        outputs[a] = np.concatenate([np.load(x) for x in file_names])\n",
    "    subprocess.call([\"rm\", \"-r\", temp_path])\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = {\n",
    "    \"local\": \"/Users/nknyazev/Documents/Delft/Thesis/temporal/data/processed/final\",\n",
    "    \"server\": \"/home/nfs/nknyazev/thesis/data/numpy\",\n",
    "    \"rtl\": \"s3://ci-data-apps/norman/sagemaker/thesis/data/processed/new/rtl/numpy\",\n",
    "}[LOCATION]\n",
    "\n",
    "output_root = {\n",
    "    \"local\": \"/Users/nknyazev/Documents/Delft/Thesis/temporal/data/results/RQ3\",\n",
    "    \"server\": \"/tudelft.net/staff-bulk/ewi/insy/MMC/nknyazev/RQ3\",\n",
    "    \"rtl\": \"s3://ci-data-apps/norman/sagemaker/thesis/offline-evaluation/RQ3\"\n",
    "}[LOCATION]\n",
    "\n",
    "data_keys = {\n",
    "    \"train\": os.path.join(DATASET if DATASET != \"rtl\" else \"\", \"train\"),\n",
    "    \"validation\": os.path.join(DATASET if DATASET != \"rtl\" else \"\", \"validation\"),\n",
    "    \"test\": os.path.join(DATASET if DATASET != \"rtl\" else \"\", \"test\"),\n",
    "}\n",
    "\n",
    "data_paths = {k: os.path.join(data_root, v) for k, v in data_keys.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy data path\n",
    "dataset_path = os.path.join(data_root, DATASET)\n",
    "# Dataset stats output path\n",
    "output_folder = os.path.join(output_root, DATASET)\n",
    "output_path = os.path.join(output_folder, \"user_stats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrays = {k: load_arrays(data_paths[k], \"X\")[\"X\"] for k in SUBSETS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {subset: [interaction1, interaction2, ..., interactionN]}\n",
    "arrays = {k:np.reshape(array, [-1, 3]) for k,array in arrays.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all padded indices (values 0,0,0) - there is no user 0\n",
    "rm_padding = lambda array: array[~np.all(array == np.array([0,0,0]), axis=1)]\n",
    "arrays = {k:rm_padding(array) for k,array in arrays.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine into one dataset\n",
    "array = np.concatenate(list(arrays.values()), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas DF\n",
    "df = pd.DataFrame(array, columns=[\"uid\", \"iid\", \"dt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group on uid\n",
    "group = df.groupby(\"uid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate stats for each user\n",
    "aggregated = group.agg({\"dt\": STATS_TO_CALCULATE})[\"dt\"].rename(lambda x: \"dt_\"+x, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df w/ index of uid and val with 1 col, containing a dict {item: times_consumed} if the item was ever consumed by uid\n",
    "aggregation = {\"iid\": lambda x: dict(x.value_counts())}\n",
    "item_consumption = df.groupby(\"uid\").agg(aggregation).rename({\"iid\": \"user_item_consumption\"}, axis=1)\n",
    "\n",
    "# Join two above dfs\n",
    "aggregated_with_consumption = aggregated.join(item_consumption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to csv\n",
    "if LOCATION != \"rtl\":\n",
    "    subprocess.call([\"mkdir\", \"-p\", output_folder])\n",
    "    aggregated_with_consumption.to_csv(output_path, sep=\"\\t\")\n",
    "else:\n",
    "    output_name = os.path.split(output_path)[-1]\n",
    "    aggregated_with_consumption.to_csv(f\"/tmp/{output_name}\", sep=\"\\t\")\n",
    "    subprocess.call([\"aws\", \"s3\", \"cp\", f\"/tmp/{output_name}\", output_path])\n",
    "    os.remove(f\"/tmp/{output_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis_p3",
   "language": "python",
   "name": "thesis_p3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
