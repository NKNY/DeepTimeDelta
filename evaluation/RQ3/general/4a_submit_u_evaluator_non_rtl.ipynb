{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run evaluation of exported models\n",
    "\n",
    "###### Execute on remote\n",
    "Load batch sizes, datasets and exporter locations, submits slurm jobs for each of them evaluating the export on validation and test sets overall. \n",
    "\n",
    "Input:\n",
    "* /home/nfs/nknyazev/thesis/data/results/exported_model_params.json. json file produced by ../2_evaluate_exports/1_gather_export_model_params or manually. This file has one model per line, containing information needed to rerun the export and is formatted as:\n",
    ">\"0\": {\n",
    "        \"dataset\": \"/home/nfs/nknyazev/thesis/data/numpy/ml-10m\",\n",
    "        \"batch_size\": \"1000\",\n",
    "        \"export_path\": \"/path/to/export/metric/000000001\"\n",
    "    }\n",
    "    \n",
    "Uses:\n",
    "* model/estimator/estimator_evaluate_export.py\n",
    "\n",
    "Returns:\n",
    "* u_evaluator_folders.json - json containing each export's paths, formatted as:\n",
    ">\"0\": {\n",
    "        \"dataset\": \"ml-10m\",\n",
    "        \"model_id\": \"0\",\n",
    "        \"path\": \"/path/to/grandparent_folder\" that would contain 3/test_u_evaluator\n",
    "    }\n",
    "    \n",
    "Note that when dealing with s3/sagemaker 4b python file should be run instead.\n",
    "\n",
    "Below `python3 model/estimator/estimator_evaluate_export.py ...` command can be adapted to match your execution environment as long as the outputs adhere to the above structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"/home/nfs/nknyazev/thesis/data/results/exported_model_params.json\"\n",
    "pickle_root = \"/tudelft.net/staff-bulk/ewi/insy/MMC/nknyazev/pickled_evaluators\"\n",
    "output_path = \"/home/nfs/nknyazev/thesis/data/results/u_evaluator_folders.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sbatch parameters\n",
    "SBATCH_STRING = \"\"\"#!/bin/sh\n",
    "#SBATCH --time=04:00:00\n",
    "#SBATCH --qos=short\n",
    "#SBATCH --cpus-per-task=2\n",
    "#SBATCH --mem=16384\n",
    "#SBATCH --gres=gpu:pascal:1\n",
    "\"\"\"\n",
    "\n",
    "# Lines needed to use tensorflow\n",
    "CUDA_STRING = \"\"\"module use /opt/insy/modulefiles\n",
    "module load cuda/10.0\n",
    "module load cudnn/10.0-7.6.0.64\n",
    "\"\"\"\n",
    "\n",
    "# String printing paremeters\n",
    "ECHO_STRING = \"echo export_path: {}\\necho data_path: {}\\necho batch_size: {}\\necho reset_devices: {}\\n\"\n",
    "\n",
    "# String for submission of \n",
    "JOB_STRING = \"srun -u python3 model/estimator/estimator_evaluate_export.py \" \\\n",
    "             \"--export_path {} \" \\\n",
    "             \"--data_path {} \" \\\n",
    "             \"--batch_size {} \" \\\n",
    "            \"--reset_devices {} \" \\\n",
    "            \"--pickle_folder {} \"\\\n",
    "            \"--pickle_non_user_evaluators False \\n \"\n",
    "\n",
    "SBATCH_FILENAME = \"job.sbatch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the params related to exports\n",
    "with open(input_path) as input_file:\n",
    "    params = json.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_history = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over exports\n",
    "for i, (k,v) in enumerate(params.items()):\n",
    "    # Actual dataset name like lastfm_10_pc and ml-10m\n",
    "    dataset = v[\"dataset\"].split(\"/\")[-1]\n",
    "    # Folder containing outliers\n",
    "    outlier_path = os.path.join(v[\"dataset\"], \"outliers\")\n",
    "    # Params included in ECHO STRING and JOB STRING\n",
    "    general_params = [v[\"export_path\"], v[\"dataset\"], v[\"batch_size\"], \"True\"]\n",
    "\n",
    "    # Write to .sbatch file\n",
    "    with open(SBATCH_FILENAME, \"w\") as output_file:\n",
    "        \n",
    "        # String allowing to retrieve params\n",
    "        echo_string = ECHO_STRING.format(*general_params)\n",
    "        job_strings = []\n",
    "        # Multiple slices of data per one sbatch file\n",
    "        # As this code is adapted from ./2_evaluate_exports/2_...ipynb it classified \n",
    "        # ALL interactions under number 3 (as opposed to numbers 0-2 for low, medium and high time gaps)\n",
    "        pickle_path = os.path.join(pickle_root, \"/\".join(general_params[0].split(\"/\")[8:18]), \"3\")\n",
    "        combined_params = general_params + [pickle_path]\n",
    "        job_strings.append(JOB_STRING.format(*combined_params))\n",
    "            \n",
    "        output_file.write(SBATCH_STRING)\n",
    "        output_file.write(CUDA_STRING)\n",
    "        output_file.write(echo_string)\n",
    "        for job_string in job_strings:\n",
    "            output_file.write(job_string)\n",
    "#     Submit the job file (requires sbatch to be installed on the system where the code is executed)\n",
    "#     subprocess.call([\"sbatch\", SBATCH_FILENAME])\n",
    "    model_id = re.findall(dataset + '/\\d{2,4}/(\\d)', v[\"export_path\"])[0]\n",
    "    pickle_parent = os.path.split(pickle_path)[0]\n",
    "    submission_history[str(i)] = {\"dataset\": dataset, \"model_id\": model_id, \"path\":pickle_parent}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_path, \"w\") as output_file:\n",
    "    json.dump(submission_history, output_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis_p3",
   "language": "python",
   "name": "thesis_p3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
