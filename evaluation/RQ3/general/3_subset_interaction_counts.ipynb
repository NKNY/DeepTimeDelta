{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Get interaction counts for each subset for each grouping for each category\n",
    "\n",
    "For specified subsets loads `X` and `seq_lens`, combines them to extract the interaction counts for each of the users for subsets separately. This information is combined with the output of `./2`, allowing to classify users to categories (e.g. `low`, `middle`,`high`) based on different groupings (e.g. mainstreamness) and calculate the number of interactions performed by those users.\n",
    "\n",
    "Requires:\n",
    "* X.npy and seq_lens.npy files for each of the dataset's subsets.\n",
    "* uid2stats.csv denoting the user membership of the $\\text{low, middle, high}$ group for each of the stats specified during the previous step.\n",
    "\n",
    "Returns:\n",
    "* subset_grouping_counts.csv containing absolute interaction counts by users belonging to the current dataset's specified subset per category for each of the groupings. Notebook also contains the percentage counts but those are not exported.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCATION = \"local\"\n",
    "DATASET = \"lastfm_10_pc\"\n",
    "SUBSETS = ['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import datetime\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from evaluation/2_evaluate_exports/RQ2.1/prev_current_dist_to_recs_vs_dt_log_bins.ipynb\n",
    "\n",
    "def randomString(stringLength=10):\n",
    "    \"\"\"Generate a random string of fixed length \"\"\"\n",
    "    letters = string.ascii_lowercase\n",
    "    return ''.join(random.choice(letters) for i in range(stringLength))\n",
    "\n",
    "def load_arrays(root, *args):\n",
    "\n",
    "    if len(args) > 0 and not root.startswith(\"s3\"):\n",
    "        return {k: np.load(os.path.join(root, k + \".npy\")) for k in args}\n",
    "    outputs = {}\n",
    "    temp_path = os.path.join(randomString())\n",
    "    subprocess.call([\"mkdir\", \"-p\", temp_path])\n",
    "    for a in args:\n",
    "        local_path = os.path.join(temp_path, a)\n",
    "        s3_path = os.path.join(root, a)\n",
    "        subprocess.call([\"mkdir\", \"-p\", local_path])\n",
    "        subprocess.call([\"aws\", \"s3\", \"cp\", s3_path, local_path, \"--recursive\"])\n",
    "        file_names = sorted([os.path.join(local_path, x) for x in next(os.walk(local_path))[-1]])\n",
    "        outputs[a] = np.concatenate([np.load(x) for x in file_names])\n",
    "    subprocess.call([\"rm\", \"-r\", temp_path])\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_root = {\n",
    "    \"local\": \"/Users/nknyazev/Documents/Delft/Thesis/temporal/data/results/RQ3\",\n",
    "    \"server\": \"/tudelft.net/staff-bulk/ewi/insy/MMC/nknyazev/RQ3\",\n",
    "    \"rtl\": \"s3://ci-data-apps/norman/sagemaker/thesis/offline-evaluation/RQ3\"\n",
    "}[LOCATION]\n",
    "\n",
    "data_root = {\n",
    "    \"local\": \"/Users/nknyazev/Documents/Delft/Thesis/temporal/data/processed/final\",\n",
    "    \"server\": \"/home/nfs/nknyazev/thesis/data/numpy\",\n",
    "    \"rtl\": \"s3://ci-data-apps/norman/sagemaker/thesis/data/processed/new/rtl/numpy\",\n",
    "}[LOCATION]\n",
    "\n",
    "output_root = {\n",
    "    \"local\": \"/Users/nknyazev/Documents/Delft/Thesis/temporal/data/results/RQ3\",\n",
    "    \"server\": \"/tudelft.net/staff-bulk/ewi/insy/MMC/nknyazev/RQ3\",\n",
    "    \"rtl\": \"s3://ci-data-apps/norman/sagemaker/thesis/offline-evaluation/RQ3\"\n",
    "}[LOCATION]\n",
    "\n",
    "data_keys = {\n",
    "    \"train\": os.path.join(DATASET if DATASET != \"rtl\" else \"\", \"train\"),\n",
    "    \"validation\": os.path.join(DATASET if DATASET != \"rtl\" else \"\", \"validation\"),\n",
    "    \"test\": os.path.join(DATASET if DATASET != \"rtl\" else \"\", \"test\"),\n",
    "}\n",
    "\n",
    "data_paths = {k: os.path.join(data_root, v) for k, v in data_keys.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_filename = \"uid2stats.csv\"\n",
    "groups_path = os.path.join(groups_root, DATASET, groups_filename)\n",
    "\n",
    "output_filename = \"subset_grouping_counts.csv\"\n",
    "output_path = os.path.join(output_root, DATASET, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load user df containing user histories\n",
    "if LOCATION != \"rtl\":\n",
    "    groups_df = pd.read_csv(groups_path, sep=\"\\t\", index_col=0)\n",
    "else:\n",
    "    tmp_folder = '/tmp'\n",
    "    _ = subprocess.call([\"aws\", 's3', 'cp', groups_path, tmp_folder])\n",
    "    tmp_path = os.path.join(tmp_folder, groups_filename)\n",
    "    groups_df = pd.read_csv(tmp_path, sep=\"\\t\", index_col=0)\n",
    "    _ = subprocess.call(['rm', tmp_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = groups_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrays = {k: load_arrays(data_paths[k], \"X\", \"seq_lens\") for k in SUBSETS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrays = {k: np.stack([v[\"X\"][:,0,0], v[\"seq_lens\"]]).T for k,v in arrays.items()}\n",
    "dfs = {k: pd.DataFrame(a, columns=[\"uid\", \"seq_lens\"]) for k,a in arrays.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_counts = {k: v.groupby(\"uid\")[\"seq_lens\"].sum() for k, v in dfs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure no users in data from X that isn't in the grouping data\n",
    "for k, v in interaction_counts.items():\n",
    "    not len(set(v.index) - set(groups_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_counts_groups = {k: pd.concat([v, groups_df], axis=1, join=\"inner\") for k,v in interaction_counts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_interaction_counts = {subset: {stat: grouping.groupby(stat)[\"seq_lens\"].sum().to_dict() for stat in stats} for subset, grouping in interaction_counts_groups.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_pandas_dict = {(subset, stat): stat_value for subset, subset_stats in group_interaction_counts.items() for stat, stat_value in subset_stats.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_df = pd.DataFrame.from_dict(counts_pandas_dict, orient=\"index\")[[\"low\", \"middle\", \"high\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_df = (counts_df.T/counts_df.sum(axis=1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as csv\n",
    "if LOCATION != \"rtl\":\n",
    "    counts_df.to_csv(output_path, sep=\"\\t\", index=True)\n",
    "else:\n",
    "    tmp_folder = \"/tmp\"\n",
    "    tmp_path = os.path.join(tmp_folder, output_filename)\n",
    "    counts_df.to_csv(tmp_path, sep=\"\\t\", index=True)\n",
    "    _ = subprocess.call([\"aws\", 's3', 'cp', tmp_path, output_path])\n",
    "    _ = subprocess.call(['rm', tmp_path])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis_p3",
   "language": "python",
   "name": "thesis_p3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
