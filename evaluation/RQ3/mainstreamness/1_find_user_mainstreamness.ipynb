{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Find user mainstreamness\n",
    "Builds the user-item matrix based on the previously extracted interaction counts. Calculates the user mainstreamness based on their similarity to the general item popularity.\n",
    "The metrics used described in https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0217389&type=printable. The obtained mainstreamness levels over all users are used to define cutoff points to classify users as low/med/high mainstreamness. These are exported as a csv with index of uid and each column indicating a particular user's mainstreamness according to a particular ranking.\n",
    "\n",
    "Requires:\n",
    "* {DATASET}/user_stats.csv generated during the previous step.\n",
    "\n",
    "Returns:\n",
    "* {DATASET}/uid2mainstreamness.csv with columns indicating each user's mainstreamness according to the column's metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCATION = \"local\"\n",
    "DATASET = \"lastfm_10_pc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import datetime\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = {\n",
    "    \"local\": \"/Users/nknyazev/Documents/Delft/Thesis/temporal/data/results/RQ3\",\n",
    "    \"server\": \"/tudelft.net/staff-bulk/ewi/insy/MMC/nknyazev/RQ3\",\n",
    "    \"rtl\": \"s3://ci-data-apps/norman/sagemaker/thesis/offline-evaluation/RQ3\"\n",
    "}[LOCATION]\n",
    "\n",
    "output_root = {\n",
    "    \"local\": \"/Users/nknyazev/Documents/Delft/Thesis/temporal/data/results/RQ3\",\n",
    "    \"server\": \"/tudelft.net/staff-bulk/ewi/insy/MMC/nknyazev/RQ3\",\n",
    "    \"rtl\": \"s3://ci-data-apps/norman/sagemaker/thesis/offline-evaluation/RQ3\"\n",
    "}[LOCATION]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET stats input path\n",
    "# user_stats_output_path = f\"/Users/nknyazev/Documents/Delft/Thesis/temporal/data/results/{DATASET}/user_stats.csv\"\n",
    "data_filename = \"user_stats.csv\"\n",
    "data_path = os.path.join(data_root, DATASET, data_filename)\n",
    "\n",
    "# User preference output path\n",
    "# output_path = f\"/Users/nknyazev/Documents/Delft/Thesis/temporal/data/results/{DATASET}/uid2mainstreamness.csv\"\n",
    "output_filename = \"uid2mainstreamness.csv\"\n",
    "output_folder = os.path.join(output_root, DATASET)\n",
    "output_path = os.path.join(output_folder, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load user df containing user histories\n",
    "if LOCATION != \"rtl\":\n",
    "    user_stats = pd.read_csv(data_path, sep=\"\\t\", index_col=0)\n",
    "else:\n",
    "    tmp_folder = '/tmp'\n",
    "    _ = subprocess.call([\"aws\", 's3', 'cp', data_path, tmp_folder])\n",
    "    tmp_path = os.path.join(tmp_folder, data_filename)\n",
    "    user_stats = pd.read_csv(tmp_path, sep=\"\\t\", index_col=0)\n",
    "    _ = subprocess.call(['rm', tmp_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find dims of a user-item matrix\n",
    "max_uid = user_stats.index.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode column from string to dict\n",
    "user_stats[\"user_item_consumption\"] = user_stats[\"user_item_consumption\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positions in 2d ui matrix to update along with the values\n",
    "updates = {(x,x2):y2 for x,y in user_stats[\"user_item_consumption\"].items() for x2,y2 in y.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iid = np.max([x[1] for x in updates.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise user item matrix\n",
    "uim = np.zeros((max_uid+1, max_iid+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing has to be all positions for one dimension in one list and all for the second list in the second one\n",
    "update_idx = list(zip(*updates.keys()))\n",
    "# Counts how many times each user consumed item from genre\n",
    "update_vals = list(updates.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the uim\n",
    "uim[update_idx] = list(update_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get consumption counts for every item as sum over all users\n",
    "item_consumption_counts = np.sum(uim, axis=0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate kendall's tau and associated p-values for every user vs global distribution\n",
    "tau_p = np.apply_along_axis(lambda x: stats.kendalltau(item_consumption_counts, x),1,  uim)\n",
    "# Split the above into two\n",
    "tau, p = [x[:,0] for x in np.split(tau_p, 2, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot kendall's Tau distribution\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "ax.set_title(f\"Kendall's Tau for MovieLens users vs global distribution {DATASET}\", fontsize=16, y=1.025)\n",
    "ax.tick_params(labelsize=12)\n",
    "ax.xaxis.label.set_size(10)\n",
    "\n",
    "ax = sns.distplot(tau[~np.isnan(tau)], kde=False, norm_hist=True, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine previous results into one dict together with the manually selected thresholds\n",
    "metrics_and_thresholds = {\n",
    "    \"Kendalls Tau\": {\n",
    "        \"metric\": tau[user_stats.index],\n",
    "        \"thresholds\": {\n",
    "            \"low\": np.percentile(tau[user_stats.index], 1/3*100),\n",
    "            \"high\": np.percentile(tau[user_stats.index], 2/3*100)\n",
    "        }\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign every user to be 0 if in left tail, 1 if in the middle and 2 if in the right tail\n",
    "user_to_metric_group = {\n",
    "    m: (d[\"metric\"] > d[\"thresholds\"][\"low\"]).astype(np.int32) + (d[\"metric\"] >= d[\"thresholds\"][\"high\"]).astype(np.int32) for m, d in metrics_and_thresholds.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back from the numeric mapping to low/middle/high and convert to df\n",
    "replacement_mapping = {0: \"low\", 1: \"middle\", 2: \"high\"}\n",
    "metric_group_df = pd.DataFrame.from_dict(user_to_metric_group).set_index(user_stats.index)\n",
    "metric_group_df_str = metric_group_df.replace({col: replacement_mapping for col in metric_group_df.columns})\n",
    "metric_group_df_str.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in metric_group_df_str.columns:\n",
    "    print(metric_group_df_str.join(user_stats.dt_count).groupby(c).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as csv\n",
    "if LOCATION != \"rtl\":\n",
    "    metric_group_df_str.to_csv(output_path, sep=\"\\t\")\n",
    "else:\n",
    "    metric_group_df_str.to_csv(tmp_path, sep=\"\\t\")\n",
    "    _ = subprocess.call([\"aws\", 's3', 'cp', tmp_path, output_path])\n",
    "    _ = subprocess.call(['rm', tmp_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis_p3",
   "language": "python",
   "name": "thesis_p3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
