{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final process of movielens dataset\n",
    "* Uses MovieLens data available [here](https://grouplens.org/datasets/movielens/10m/) at the time of writing.\n",
    "* Generate file assigning uid to original uid: uid_to_uid.csv\n",
    "* Generate file assigning iid to original artist-song pair: iid_to_movie_genre.csv\n",
    "* Generate 3 folders: train, validation, test\n",
    "* Each of the folders should in the end have X.npy, y.npy, seq_lens.npy, user_ids.npy (the last one is not explicitly needed for training but may be useful for debugging)\n",
    "* The order of things is as follows:\n",
    "    * Assign unique ids to users and items (keep track of original values - make one column for movie+genre)\n",
    "    * Convert time to unix epochs\n",
    "    * Remove users with 2 or fewer interactions\n",
    "    * Sort each user's interaction by time so that the first thing that happened is also placed first, break ties nonrandomly\n",
    "    * Add delta_t by removing the first interaction for each user\n",
    "    * Make remaining items to be sequential - record to iid_to_movie_genre.csv\n",
    "        * Do that only after removing items that are not in train. This has to be done a high % of items are removed from val-test - they would be random noise that would also lead to possibly noticeable memory waste in the embedding matrix. Thus from the original DataFrame all unique iid-movie-genre combinations are obtained, joined together with the train_df from which all the unique items used in the experiments are obtained. A new factorised column is created and the conversion from these new indices to relevant movie-genre pairs is made. The new table is then joined with the train_df, val_df and test_df again on iid and thus new index is supplied to these DataFrames.\n",
    "    * Split into train-validation-test with overhangs of one item (for label)\n",
    "        * Apply same logic as in dataset.py\n",
    "    * for each subset:\n",
    "        * Remove items from validation and test if they are not present in train\n",
    "        * Split into X,y\n",
    "        * Place into numpy arrays 20 interactions at a time, apply padding if needed\n",
    "        * Obtain seq_lens and user_ids\n",
    "        * Save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML\n",
    "from itertools import compress\n",
    "import sys\n",
    "import os\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = Path(\"/Users/nknyazev/Documents/Delft/Thesis/temporal\") # Specify your own project root\n",
    "data_root = project_root.joinpath(\"data\")\n",
    "code_root = project_root.joinpath(\"code\")\n",
    "input_path = data_root.joinpath(\"original/ml-10m/ratings.dat\")\n",
    "iid_movie_genre_path = data_root.joinpath(\"original/ml-10m/movies.dat\")\n",
    "output_dir = data_root.joinpath(\"processed/final/ml-10m/\")\n",
    "input_columns = [\"uid\", \"iid\", \"rating\", \"t\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional imports from own modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(str(code_root))\n",
    "import model.utils.datasplit\n",
    "reload(model.utils.datasplit)\n",
    "from model.utils.datasplit import train_val_test_split_train_overlapping, remove_unseen_items_in_train, generate_big_hop_numpy_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset as pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(input_path, names=input_columns, sep=\"::\", header=None, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of unique users: {}\".format(df[\"uid\"].nunique()))\n",
    "print(\"Number of unique items: {}\".format(df[\"iid\"].nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=\"rating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All interactions grouped by user id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(\"uid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No users with fewer than 3 interactions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each item is a list containing indices of interactions belonging to a short user\n",
    "cols_for_users_under_3 = [grouped.groups[k] for k in grouped.groups.keys() if len(grouped.groups[k]) < 3]\n",
    "# Rows in DataFrame to remove\n",
    "flattened = [idx for user in cols_for_users_under_3 for idx in user]\n",
    "print(\"Found {} short users summing to {} interactions\".format(len(cols_for_users_under_3), len(flattened)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort each user in time (the ones happened longer ago first). Break ties non-randomly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to reorder each user\n",
    "Breaks ties non-randomly. First tries to order interactions by time (ascending) - many interactions were logged at the same exact time and thus using a second column (DataFrame index) to break ties - the item that appeared first in the file is also placed first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index().sort_values(by=[\"uid\", \"t\", \"index\"]).drop(columns=\"index\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the time is now sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head())\n",
    "print(\"Time is sorted in correct order\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate delta_t's "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array to keep track of indices of 1st interaction for each user - these indices will be removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_interaction_indices = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array to keep track of time deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_deltas = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process each user - this should produce the same total number of interactions but each user's first interaction will have NaN in time_deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped = df.groupby(\"uid\")\n",
    "for uid, interactions in grouped:\n",
    "    if len(interactions) > 2:\n",
    "        remove_interaction_indices.append(interactions.index[0])\n",
    "        time_delta_with_na = interactions[\"t\"] - interactions.shift(1)[\"t\"]\n",
    "        time_deltas.extend(time_delta_with_na)\n",
    "    else:\n",
    "        remove_interaction_indices.extend(interactions.index)\n",
    "        print(\"Removed interactions directly.\")\n",
    "    if uid % 50 == 1:\n",
    "        print(\"Completed user {}.\".format(uid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove nan's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_deltas_wo_na = list(compress(time_deltas, ~np.isnan(time_deltas)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(time_deltas_wo_na)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check: len of original df - number of nan's = len of new df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df) - len(remove_interaction_indices) == len(time_deltas_wo_na)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove interactions without time deltas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(remove_interaction_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add time deltas to output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"dt\"] = np.array(time_deltas_wo_na, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(df[\"dt\"] == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into three dataframes: train, validation, test - 0.9, 0.05, 0.05 of each user's sequence respectively\n",
    "`train_val_test_split_train_overlapping` from `model.utils.datasplit` of this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df = train_val_test_split_train_overlapping(df=df[[\"uid\", \"iid\", \"dt\"]], \n",
    "                                                                   col_names=[\"uid\", \"iid\", \"dt\"],\n",
    "                                                                  split=[0.9, 0.05, 0.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original DataFrame Length - {}\\nResulting DataFrame lengths:\\nTrain - {}\\nValidation - {}\\nTest - {}\\nTotal lengths - {}\".format(len(df), len(train_df), len(val_df), len(test_df), len(train_df)+len(val_df)+len(test_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For test/eval remove interactions with items not present in train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_val_items = set(val_df[\"iid\"])\n",
    "og_ts_items = set(test_df[\"iid\"])\n",
    "og_val_ts_items = og_val_items.union(og_ts_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = remove_unseen_items_in_train(train_df=train_df, test_df=val_df)\n",
    "test_df = remove_unseen_items_in_train(train_df=train_df, test_df=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_items = set(val_df[\"iid\"])\n",
    "ts_items = set(test_df[\"iid\"])\n",
    "val_ts_items = val_items.union(ts_items)\n",
    "items_removed = len(og_val_ts_items)-len(val_ts_items)\n",
    "items_in_original_df = df[\"iid\"].nunique()\n",
    "print(\"Removed {} unique items from train and validation, which is {} of the original dataset's items.\".format(items_removed,round(items_removed/items_in_original_df, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original DataFrame Length - {}\\nResulting DataFrame lengths:\\nTrain - {}\\nValidation - {}\\nTest - {}\\nTotal lengths - {}\".format(len(df), len(train_df), len(val_df), len(test_df), len(train_df)+len(val_df)+len(test_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need to create linkage between item indices in train_df and external file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(iid_movie_genre_path, header=None, names=[\"iid\", \"movie_name\", \"genre\"], sep=\"::\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame with unique items as indices, uid, t as values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_iid_train_df = train_df.groupby(\"iid\").first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the two above on the iid index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_iid_movie_genre = unique_iid_train_df.join(df2.set_index(\"iid\"))[[\"movie_name\", \"genre\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new column with factorized iid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique_iid_movie_genre[\"new_iid\"] = pd.factorize(unique_iid_movie_genre.index)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_iid_movie_genre.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data to a separate file containing explanations what artist-song pair each item id stands for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify output path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_iid_movie_genre_path = output_dir.joinpath(\"iid_to_movie_genre.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_iid_movie_genre.to_csv(unique_iid_movie_genre_path, columns=[\"new_iid\", \"movie_name\", \"genre\"], header=False, index=False, sep=\"\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join each of the train/validation/test DataFrames on iid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.join(unique_iid_movie_genre[[\"new_iid\"]], on=\"iid\").drop(\"iid\", axis=1).rename(columns={\"new_iid\": \"iid\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = val_df.join(unique_iid_movie_genre[[\"new_iid\"]], on=\"iid\").drop(\"iid\", axis=1).rename(columns={\"new_iid\": \"iid\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify before-after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.join(unique_iid_movie_genre[[\"new_iid\"]], on=\"iid\").drop(\"iid\", axis=1).rename(columns={\"new_iid\": \"iid\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create X, y, seq_lens and user_ids out of these three DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_array = []\n",
    "val_array = []\n",
    "test_array = []\n",
    "dfs = [train_df, val_df, test_df]\n",
    "arrays = [train_array, val_array, test_array]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate over each of the three DataFrames and create 4 numpy arrays that are added to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index in range(len(arrays)):\n",
    "    dataframe = dfs[index]\n",
    "    X, y, seq_lens = generate_big_hop_numpy_files(dataframe, features=[\"uid\", \"iid\", \"dt\"], save=False)\n",
    "    arrays[index].extend([X, y, seq_lens, X[:,0,0]])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save these into output_dir/{subset} as .npy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_names = [\"train\", \"validation\", \"test\"]\n",
    "file_types = [\"X\", \"y\", \"seq_lens\", \"user_ids\"]\n",
    "file_names = [x + \".npy\" for x in file_types]\n",
    "for x in range(len(subset_names)):\n",
    "    target_folder = output_dir.joinpath(subset_names[x])\n",
    "    try:\n",
    "        os.mkdir(str(target_folder))\n",
    "    except FileExistsError:\n",
    "        print(\"Folder {} already exists.\".format(str(target_folder)))\n",
    "    for y in range(len(arrays[x])):\n",
    "        file_path = str(target_folder.joinpath(file_names[y]))\n",
    "        print(\"Writing {}\".format(file_path))\n",
    "        np.save(file_path, arrays[x][y])\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis_p3",
   "language": "python",
   "name": "thesis_p3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
