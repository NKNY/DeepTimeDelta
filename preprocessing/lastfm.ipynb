{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing LastFM dataset\n",
    "* Uses LastFM data available [here](http://ocelma.net/MusicRecommendationDataset/lastfm-1K.html) at the time of writing.\n",
    "* 10 percent sampling\n",
    "* Generate file assigning uid to original uid: uid_to_uid.csv\n",
    "* Generate file assigning iid to original artist-song pair: iid_to_artistsong.csv\n",
    "* Generate 3 folders: train, validation, test\n",
    "* Each of the folders should in the end have X.npy, y.npy, seq_lens.npy, user_ids.npy (the last one is not explicitly needed for training but may be useful for debugging)\n",
    "* The order of things is as follows:\n",
    "    * Assign unique ids to users and items (keep track of original values - make one column for artist+song)\n",
    "    * Convert time to unix epochs\n",
    "    * Remove users with 2 or fewer interactions\n",
    "    * Sort each user's interaction by time so that the first thing that happened is also placed first, break ties deterministically\n",
    "    * Add delta_t by removing the first interaction for each user\n",
    "    * Make remaining items to be sequential - record to iid_to_song.csv\n",
    "        * Do that only after removing items that are not in train. This has to be done as 5% of items are removed from val-test - they would be random noise that would also lead to possibly noticeable memory waste in the embedding matrix. Thus from the original DataFrame all unique iid-artist-song combinations are obtained, joined together with the train_df from which all the unique items used in the experiments are obtained. A new factorised column is created and the conversion from these new indices to relevant artist-song pairs is made. The new table is then joined with the train_df, val_df and test_df again on iid and thus new index is supplied to these DataFrames.\n",
    "    * Split into train-validation-test with overhangs of one item (for label)\n",
    "        * Apply same logic as in dataset.py\n",
    "    * for each subset:\n",
    "        * Remove items from validation and test if they are not present in train\n",
    "        * Split into X,y\n",
    "        * Place into numpy arrays 20 interactions at a time, apply padding if needed\n",
    "        * Obtain seq_lens and user_ids\n",
    "        * Save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML\n",
    "from itertools import compress\n",
    "import sys\n",
    "import os\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = Path(\"/Users/nknyazev/Documents/Delft/Thesis/temporal\") # Specify your own project root\n",
    "data_root = project_root.joinpath(\"data\")\n",
    "code_root = project_root.joinpath(\"code\")\n",
    "input_path = data_root.joinpath(\"original/lastfm-dataset-1K/userid-timestamp-artid-artname-traid-traname.tsv\")\n",
    "output_dir = data_root.joinpath(\"processed/final/lastfm_10_pc/\")\n",
    "input_columns = [\"og_user\", \"og_time\", \"artist_code\", \"artist\", \"song_code\", \"song\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional imports from own modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(str(code_root))\n",
    "import model.utils.datasplit\n",
    "reload(model.utils.datasplit)\n",
    "from model.utils.datasplit import train_val_test_split_train_overlapping, remove_unseen_items_in_train, generate_big_hop_numpy_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset as pandas DataFrame\n",
    "As the file is encoded in a weird way Pandas discards lines.\n",
    "So reading of the file is performed in a more manual way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "with open(input_path) as input_file:\n",
    "    for line in input_file:\n",
    "        split = line.rstrip().split(\"\\t\")\n",
    "        lines.append(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(lines, columns=input_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clear up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign unique ids to users and songs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"uid\"] = [int(x[5:]) for x in df[\"og_user\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"iid\"] = df.groupby([\"artist\", \"song\"]).ngroup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intermediary results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of unique users: {}\".format(df[\"uid\"].nunique()))\n",
    "print(\"Number of unique items: {}\".format(df[\"iid\"].nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove users with 2 or fewer interactions\n",
    "One interaction is needed for delta_t<br/>\n",
    "One interaction is needed to produce a label<br/>\n",
    "At least one more interaction is needed to have an entry in X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All interactions grouped by user id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(\"uid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find indices for all interactions for all users who have too few interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each item is a list containing indices of interactions belonging to a short user\n",
    "cols_for_users_under_12 = [grouped.groups[k] for k in grouped.groups.keys() if len(grouped.groups[k]) < 12]\n",
    "# Rows in DataFrame to remove\n",
    "flattened = [idx for user in cols_for_users_under_12 for idx in user]\n",
    "print(\"Found {} users summing to {} interactions\".format(len(cols_for_users_under_12), len(flattened)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the specified rows from the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(flattened)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort each user in time (the ones happened longer ago first). Break ties non-randomly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All interactions grouped by user id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(\"uid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert time into unix epoch format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert iso date to unix epoch\n",
    "def iso_to_epoch(iso):\n",
    "    datetime_object = datetime.datetime.strptime(iso, '%Y-%m-%dT%H:%M:%SZ')\n",
    "    return int((datetime_object.timestamp())) + 7200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column with unix timestamp for each interaction\n",
    "epoch_col = [iso_to_epoch(x) for x in list(df[\"og_time\"])]\n",
    "df[\"t\"] = epoch_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head())\n",
    "print(\"Time is sorted but in the reverse order\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop unnecessary columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"og_user\", \"og_time\", \"artist_code\", \"song_code\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reordering each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While this reordering is faster,\n",
    "# in case of zero gaps items are (likely) entered from most recent to the oldest and sorting using index will\n",
    "# be different from inverting each user's history.\n",
    "# df = df.reset_index().sort_values(by=[\"uid\", \"t\", \"index\"]).drop(columns=\"index\").reset_index(drop=True)\n",
    "\n",
    "grouped = df.groupby(\"uid\")\n",
    "new_index_col = []\n",
    "for uid, items in grouped:\n",
    "    new_indices = items.index[::-1]\n",
    "    new_index_col.extend(new_indices)\n",
    "    if uid % 100 == 1:\n",
    "        print(\"Processed user {}.\".format(uid))\n",
    "df.index = new_index_col\n",
    "df = df.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the time is now sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTIONAL: Sampling done so that user retains 10% steps where predictions are made - to minimise discarding of users if a user 12 items and we have to pick 3 (one used for delta_t shift, one for features and the last for label) - one could view that we have 10 possible starting points for this sequence of three - sufficient to sample 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "grouped = df.groupby(\"uid\")\n",
    "indices_to_keep = []\n",
    "for uid, interactions in grouped:\n",
    "    num_interactions = len(interactions)\n",
    "    post_sample_size = int(np.floor(0.1 * (num_interactions-2))) + 2\n",
    "    \n",
    "    last_allowed_index = num_interactions-post_sample_size\n",
    "    sample_start = np.random.randint(0, last_allowed_index+1, 1)[0]\n",
    "    sample_end = sample_start + post_sample_size\n",
    "    user_indices_to_keep = interactions.index[sample_start:sample_end]\n",
    "    indices_to_keep.extend(user_indices_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[indices_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resulting average user sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([len(x) for uid, x in df.groupby(\"uid\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate delta_t's "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array to keep track of indices of 1st interaction for each user - these indices will be removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_interaction_indices = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array to keep track of time deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_deltas = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process each user - this should produce the same total number of interactions but each user's first interaction will have NaN in time_deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(\"uid\")\n",
    "for uid, interactions in grouped:\n",
    "    if len(interactions) > 2:\n",
    "        remove_interaction_indices.append(interactions.index[0])\n",
    "        time_delta_with_na = interactions[\"t\"] - interactions.shift(1)[\"t\"]\n",
    "        time_deltas.extend(time_delta_with_na)\n",
    "    else:\n",
    "        remove_interaction_indices.extend(interactions.index)\n",
    "        print(\"Removed interactions directly.\")\n",
    "    if uid % 50 == 1:\n",
    "        print(\"Completed user {}.\".format(uid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove nan's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_deltas_wo_na = list(compress(time_deltas, ~np.isnan(time_deltas)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check: len of original df - number of nan's = len of new df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df) - len(remove_interaction_indices) == len(time_deltas_wo_na)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove interactions without time deltas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(remove_interaction_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add time deltas to output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"dt\"] = np.array(time_deltas_wo_na, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles_to_consider = sorted([0.5, 99.5] + list(range(1,100)))\n",
    "percentiles = {x:np.percentile(df[\"dt\"], x) for x in percentiles_to_consider}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in percentiles.items():\n",
    "    delta_t = str(int(y)) + \" seconds\" if y < 60 else str(round(y/60, 1)) + \" minutes (\" \\\n",
    "    + str(round(y/60/60, 1)) + \" hours)\"\n",
    "    count = np.sum(df[\"dt\"] <= y) if x < 50 else np.sum(df[\"dt\"] >= y)\n",
    "    print(\"Percentile: {} - {}. {} interactions\".format(x, delta_t, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See how current progress looks like - can manually inspect that so far completed correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into three dataframes: train, validation, test - 0.9, 0.05, 0.05 of each user's sequence respectively\n",
    "\n",
    "`train_val_test_split_train_overlapping` from `model.utils.datasplit` of this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df = train_val_test_split_train_overlapping(df=df[[\"uid\", \"iid\", \"dt\"]], \n",
    "                                                                   col_names=[\"uid\", \"iid\", \"dt\"],\n",
    "                                                                  split=[0.9, 0.05, 0.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original DataFrame Length - {}\\nResulting DataFrame lengths:\\nTrain - {}\\nValidation - {}\\nTest - {}\\nTotal lengths - {}\".format(len(df), len(train_df), len(val_df), len(test_df), len(train_df)+len(val_df)+len(test_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For test/eval remove interactions with items not present in train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_val_items = set(val_df[\"iid\"])\n",
    "og_ts_items = set(test_df[\"iid\"])\n",
    "og_val_ts_items = og_val_items.union(og_ts_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = remove_unseen_items_in_train(train_df=train_df, test_df=val_df)\n",
    "test_df = remove_unseen_items_in_train(train_df=train_df, test_df=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_items = set(val_df[\"iid\"])\n",
    "ts_items = set(test_df[\"iid\"])\n",
    "val_ts_items = val_items.union(ts_items)\n",
    "items_removed = len(og_val_ts_items)-len(val_ts_items)\n",
    "items_in_original_df = df[\"iid\"].nunique()\n",
    "print(\"Removed {} unique items from train and validation, which is {} of the original dataset's items.\".format(items_removed,round(items_removed/items_in_original_df, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need to create linkage between indices in train_df and original df\n",
    "* Get unique item id's from train_df\n",
    "* Join these unique id's with original dataframe's iid-artist-song slice\n",
    "* Factorize on iid\n",
    "* Save factorized iid, artist, song as csv\n",
    "* Join factorized iid, old iid with train_df, val_df, test_df. Then drop old iid column from each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame with unique items as indices, uid, t as values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_iid_train_df = train_df.groupby(\"iid\").first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame with unique items as indices, artist, song name as values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_iid_artist_song = df[[\"iid\", \"artist\", \"song\"]].groupby(\"iid\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_iid_artist_song.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the two above on the iid index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_iid_artist_song_uid_t = unique_iid_train_df.join(unique_iid_artist_song)[[\"artist\", \"song\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new column with factorized iid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_iid_artist_song_uid_t.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique_iid_artist_song_uid_t[\"new_iid\"] = pd.factorize(unique_iid_artist_song_uid_t.index)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_iid_artist_song_uid_t.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data to a separate file containing explanations what artist-song pair each item id stands for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify output path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iid_to_artistsong_path = output_dir.joinpath(\"iid_to_artistsong.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_iid_artist_song_uid_t.to_csv(iid_to_artistsong_path, columns=[\"new_iid\", \"artist\", \"song\"], header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join each of the train/validation/test DataFrames on iid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.join(unique_iid_artist_song_uid_t[[\"new_iid\"]], on=\"iid\").drop(\"iid\", axis=1).rename(columns={\"new_iid\": \"iid\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = val_df.join(unique_iid_artist_song_uid_t[[\"new_iid\"]], on=\"iid\").drop(\"iid\", axis=1).rename(columns={\"new_iid\": \"iid\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify before-after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.join(unique_iid_artist_song_uid_t[[\"new_iid\"]], on=\"iid\").drop(\"iid\", axis=1).rename(columns={\"new_iid\": \"iid\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the last time gap to csv - needed for session based analysis\n",
    "sufficient_interaction_mask = test_df.groupby(\"uid\").uid.transform(\"count\") > 1\n",
    "test_df[sufficient_interaction_mask].groupby(\"uid\")[\"uid\", \"dt\"].tail(1).to_csv(output_dir.joinpath(\"last_dt.csv\"), index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create X, y, seq_lens and user_ids out of these three DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_array = []\n",
    "val_array = []\n",
    "test_array = []\n",
    "dfs = [train_df, val_df, test_df]\n",
    "arrays = [train_array, val_array, test_array]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate over each of the three DataFrames and create 4 numpy arrays that are added to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(arrays)):\n",
    "    dataframe = dfs[index]\n",
    "    X, y, seq_lens = generate_big_hop_numpy_files(dataframe, features=[\"uid\", \"iid\", \"dt\"], save=False)\n",
    "    arrays[index].extend([X, y, seq_lens, X[:,0,0]])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save these into output_dir/{subset} as .npy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_names = [\"train\", \"validation\", \"test\"]\n",
    "file_types = [\"X\", \"y\", \"seq_lens\", \"user_ids\"]\n",
    "file_names = [x + \".npy\" for x in file_types]\n",
    "for x in range(len(subset_names)):\n",
    "    target_folder = output_dir.joinpath(subset_names[x])\n",
    "    try:\n",
    "        os.mkdir(str(target_folder))\n",
    "    except FileExistsError:\n",
    "        print(\"Folder {} already exists.\".format(str(target_folder)))\n",
    "    for y in range(len(arrays[x])):\n",
    "        file_path = str(target_folder.joinpath(file_names[y]))\n",
    "        print(\"Writing {}\".format(file_path))\n",
    "        np.save(file_path, arrays[x][y])\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis_p3",
   "language": "python",
   "name": "thesis_p3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16.0,
    "lenType": 16.0,
    "lenVar": 40.0
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
